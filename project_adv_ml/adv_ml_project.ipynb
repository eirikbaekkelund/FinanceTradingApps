{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Recurrent Learning (N-BEATS) vs. Traditional Machine Learning\n",
    "## Advanced Machine Learning in Finance - Individual Project\n",
    "\n",
    "The aim of this project is to compare the performance of two machine learning models, N-Beats and RandomForests, for predicting sales figures. The challenge is to use available data that has a correlation with the sales of various companies, such as credit card transactions to predict the quarterly sales figures.\n",
    "\n",
    "The goal is to compare the accuracy of predictions for the current quarter, as well as the next couple of quarters, between N-Beats and RandomForests.\n",
    "\n",
    "N-Beats is a deep learning model designed specifically for time-series forecasting, while RandomForests is a popular tree-based model used for various types of predictive modeling. This project aim to give insights into comparative forecasting results from these two models and their applications in the field of sales prediction.\n",
    "\n",
    "The outcome of this project will be a comparison between the two models, including a discussion of the results and their implications for businesses and organizations looking to improve their financial forecasting capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import darts\n",
    "import numpy as np\n",
    "import data_wrangling as dw\n",
    "import time_series_preprocessing as tsp\n",
    "import matplotlib.pyplot as plt\n",
    "import visualisations as vis\n",
    "from darts.models import NBEATSModel\n",
    "from darts.models import BlockRNNModel, RandomForest, XGBModel\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from darts.metrics import mape, rmse\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue = dw.create_df('revenue.xlsx')\n",
    "spendings = dw.create_df('spend_amounts_aggregated.xlsx')\n",
    "df = dw.merge_dataframes(df_left=spendings, df_right=revenue)\n",
    "df = dw.encode_index(df, 'mic')\n",
    "df = dw.add_time_cols(df)\n",
    "df = dw.remove_short_series(df)\n",
    "df = dw.remove_missing_ground_truth(df)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of companies\n",
    "len(df['ticker'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw.print_nans_companies(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the vast majority of missing vals are happening in the column 'nw_total_sales_b_total' for the indices in some form of subset of [0,1,2,3] so that it makes sense to impute these values to avoid missing large amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_scatter_log(df,'nw_total_sales_a_total', 'nw_total_sales_b_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_hist(df, 'nw_total_sales_a_total','nw_total_sales_b_total' )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distributions are quite similar but shifted, and that they have a fairly linear relationship when we apply the log. Thus, a linear least squares approach is justified. Including the other columns give a higher degree of freedom, but it could be justified to just use these columns up against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dw.impute_nans_singular_column(df, col='nw_total_sales_b_total' ,plot=True, max_plots=3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plots a LLS solution looks sensible to the following time series, and thus handles many of the missing datapoints. The accuracy of these are however unknown. Bayesian regression can be performed to quantify uncertainty on the imputation on the missing values. To avoid noisy inputs, one could also perform lasso/ridge regression to shrink mutual information features and reduce high variance issues.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Actual and Estimated sales for fiscal quarters have a decent amount of NaN values. Thus, we can statistically inspect them as well, and see if there are favorable methods of imputation of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_hist(df, 'Sales_Actual_fiscal', 'Sales_Estimate_fiscal')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions have a very large overlap. Note that the distribution is a concetanation of all companies, so even though this looks normal, it is not necessarily the case for the distribution of actual sales and estimated sales with respect to the company in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_scatter_log(df, 'Sales_Actual_fiscal', 'Sales_Estimate_fiscal')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this it looks as they are very linearly correlated. Let's inspect how it looks like for respective companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_sales_comparison(df, max_plots=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that there are many instances for which the fiscal sales are missing for both estimate and actual, for that reason, we should apply an algorithm that can impute the missing values well. When both have NaNs at the same index over a time span ~5 there are uncertainty with doing rolling averages as there are fluctuations between quarters for many companies. The suggested idea would be to that of a KNN or KD-Ball algorithm to replace these instances. However, for the instances when there are rows when one or the other is present, it makes sense to impute values based on the other. Applying a normal centered at the no NaN column value at the corresponding index with a standard deviation that is the square root of the absolute difference of numeric rows seems like a intuitive and good solution based on their distribution. Also, when there is just a singular column of these with NaN values after doing the computations, we can perform LLS like earlier on those columns, given linearity exists in the dataset for these instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dw.fiscal_sales_imputation(df, plot=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many of the total sales in A and B these are overlapping. A strategy here could be to use KNN / KD-Balls, Decision Trees or other ML algorithms for imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw.print_nans_companies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dw.impute_nans_singular_column(df, col='nw_total_sales_b_total',plot=True, max_plots=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can seem as this is cause for some anomolies when performing LLS. Let's create a pipeline to detect anomolies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ticker.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dw.impute_nans_singular_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any')\n",
    "df = dw.remove_short_series(df)\n",
    "print(df.shape, '\\n')\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating new columns with high correlation to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dw.add_proportion_ab(df)\n",
    "df = dw.add_quarterly_yoy(df)\n",
    "df = dw.add_prod(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ticker.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_correlation_matrix(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **key** to understanding here is that the high uncorrelation comes from the fact that we have a dataframe containing companies in different market segments with completely different scales to their sales by season, size of company etc., so looking at the whole dataframe gives a misleading correlation to the actual ground truth. If we take a single company, we can see that the correlation between columns will change drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = 'AMZN'\n",
    "df_brand = df[df['ticker'] == brand]\n",
    "vis.plot_correlation_matrix(df_brand)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the removal of columns is done by the respective ticker. If the overall mean covariance across all companies is below a threshold (set to 0.1), we remove it. Luckily for us there seem to be a very high correlation between actual sales to all other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.groupby('ticker').apply(lambda x: x.corrwith(x['Sales_Actual_fiscal'], numeric_only=True)).mean()\n",
    "corr = corr.sort_values(ascending=False)\n",
    "corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to drop low correlation features\n",
    "df = dw.drop_low_correlation_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand = df[df.ticker == 'AMZN']\n",
    "vis.plot_correlation_matrix(df_brand)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impact of covid and other macro economic events can have a significant impact on the sales. This we see in the reflection of the year's correlation with sales. An underlying factor for why this is so high is probably due to the effect of pre-mid-post COVID."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Series in Darts\n",
    "Note that the data frame must be thoroughly processed before starting this process, and keep in mind to that the model actually does handle NaNs implicitly. So, the thorough removal may not be neccessary. Especially, the line when all NaNs are dropped. Darts also has it's own models for imputations, but due to the structure of the data in this project, manual imputation using simple statistics was done."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we create time series in darts, our model can support stationary covariates. As there is significant differences between companies in their sales volume, seasonalities, etc. we include this information to the model we are about to create. Thus, we must encode the stationary covariates, which we can do by mapping the tickers to a numeric value. To avoid numeric instability to the mapping, we do not do integer encoding as we have >200 companies to evaluate. This is to avoid any arbitrary ordering of assigning integer values. To have the model capture the stationary covariates there are two alternatives that seem reasonable:  \n",
    "  \n",
    "1. One Hot Encoding:   \n",
    "Represent the tickers as one hot encoded covariates added to the function.\n",
    "  \n",
    "2. Mapping:  \n",
    "Map the company to a range of somewhat similar float values to avoid numerical instability \n",
    " \n",
    "Now, because the number of companies exceeds the number of rows by a significant amount, it is uncertain whether the network will carry an advantage of adding so many stationary covariates. Thus, I have gone with option 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, inv_mapper = dw.encode_float(df, 'ticker')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions above help extract appropriate covariates and target for each company within the data frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the data frame into series for each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_dict = tsp.dict_series(df, inv_mapper)\n",
    "past_cov, future_cov, target, tickers = tsp.past_future_split(series_dict, n_preds=2)\n",
    "past_cov, future_cov, target, tickers = tsp.match_input_length(past_cov, future_cov, target, tickers, min_train = 12)\n",
    "past_cov, future_cov, target, scaler_cov, scaler_target = tsp.scale_series(past_cov, future_cov, target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below line all lists have the corresponding index to the respective company.\n",
    "I.e., If we look at;  \n",
    "\n",
    " ```train_past_cov[i], train_target[i], train_future_cov[i], tickers_train[i], scaler_cov_train[i], scaler_target_train[i]```\n",
    "\n",
    "It corresponds to the same company $\\forall i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_past_cov, test_past_cov, \\\n",
    "train_future_cov, test_future_cov, \\\n",
    "train_target, test_target, \\\n",
    "tickers_train, tickers_test, \\\n",
    "scaler_cov_train, scaler_cov_test, \\\n",
    "scaler_target_train, scaler_target_test \\\n",
    "= tsp.train_test_split(\n",
    "    past_cov,  # input feature data for past time steps in the training set\n",
    "    future_cov,  # input feature data for future time steps in the training set\n",
    "    target,  # target variable data in the training set\n",
    "    tickers,  # ticker symbols for the data in the training set\n",
    "    scaler_cov,  # scaler object to normalize the input feature data in the training set\n",
    "    scaler_target,  # scaler object to normalize the target variable data in the training set\n",
    "    test_size=0.2  # the proportion of the data to be used for testing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove points to predict from train and test data\n",
    "train_target_input = tsp.remove_n_predictions(train_target, train_future_cov)\n",
    "test_target_input = tsp.remove_n_predictions(test_target, test_future_cov)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Beats settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300                        # number of epochs\n",
    "n_stacks = 3                        # number of stacks in model\n",
    "n_blocks = 9                        # number of blocks in model        \n",
    "layer_width = 128                   # numer of weights in FC layer\n",
    "lr = 1e-3                           # learning rate\n",
    "val_wait = 1                        # epochs to wait before evaluating the loss on the test/validation set\n",
    "seed = 42                           # random seed for regenerating results\n",
    "n_jobs = -1                         # parallel processors to use;  -1 = all processors\n",
    "input_length = len(past_cov[0])     # length of the input sequence\n",
    "output_length = len(future_cov[0])  # length of the output sequence\n",
    "\n",
    "# quantiles for QuantileRegression argument\n",
    "quantiles = [0.01, 0.1, 0.2, 0.5, 0.8, 0.9, 0.99]\n",
    "\n",
    "# lower and upper quantiles for predictions\n",
    "QL1, QL2 = 0.01, 0.05 \n",
    "QU1, QU2 = 1 - QL1, 1 - QL2 \n",
    "# labels for plotting\n",
    "labelQ1 = f'{int(QU1 * 100)} / {int(QL1 * 100)} percentile band'\n",
    "labelQ2 = f'{int(QU2 * 100)} / {int(QL2 * 100)} percentile band'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nbeats = NBEATSModel(input_chunk_length=len(train_past_cov[0]), \n",
    "                     output_chunk_length=len(train_future_cov[0]),\n",
    "                     num_stacks=n_stacks,\n",
    "                     num_blocks=n_blocks,\n",
    "                     layer_widths=layer_width,\n",
    "                     n_epochs=epochs,\n",
    "                     likelihood = QuantileRegression(quantiles),\n",
    "                     optimizer_kwargs = {\"lr\" : 1e-3},\n",
    "                     generic_architecture=True,\n",
    "                     trend_polynomial_degree=2,\n",
    "                     random_state=seed,\n",
    "                     nr_epochs_val_period=val_wait,\n",
    "                     #pl_trainer_kwargs = {\"accelerator\": \"cpu\", \"devices\": 8 } \n",
    "                     )\n",
    "predictor_nbeats = model_nbeats.fit(series=train_target, \n",
    "                       past_covariates=train_past_cov,\n",
    "                       verbose=False,\n",
    "                       epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make a function for predicting using darts predictor - see docs\n",
    "pred_train = predictor_nbeats.predict(n=2,\n",
    "                                series=train_target_input,\n",
    "                                past_covariates=train_past_cov,\n",
    "                                n_jobs=-1,\n",
    "                                mc_dropout=True,\n",
    "                                )\n",
    "\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predictor_nbeats.predict(n=2,\n",
    "                                series=test_target_input,\n",
    "                                past_covariates=test_past_cov,\n",
    "                                n_jobs=-1,\n",
    "                                mc_dropout=True,\n",
    "                                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the uncertainty bands do not represent anything. By retraining the model n times \"large enough\" for assuming normality in the predictions we can assert an uncertainty band to the predictions. This is omitted due to the training time involved in doing so."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traininig plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_predictions(pred_train, train_target, scaler_target_train, tickers_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_predictions(pred_test, test_target, scaler_target_test, tickers_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "\n",
    "### N-Beats / RNN (Architecture Design)  \n",
    "https://unit8co.github.io/darts/examples/17-hyperparameter-optimization.html\n",
    "* n_layers\n",
    "* n_stacks\n",
    "* input_chunk_length (lookback window of the model)\n",
    "* output_chunk_length (how many steps into the future we make predictions)\n",
    "* n past covariates to use\n",
    "* n epochs in training\n",
    "* layer width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.tensor([rmse(target[-2:], pred) for target, pred in zip(train_target, pred_train)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective_nbeats(params, train_target, train_past_cov, seed=42):\n",
    "    \"\"\" \n",
    "    Optimization function for hyperparameter tuning\n",
    "\n",
    "    Args:\n",
    "        params (tuple): hyperparameters to optimize\n",
    "        train_target (TimeSeries): training target\n",
    "        train_past_cov (TimeSeries): training past covariates\n",
    "        val_target_input (TimeSeries): validation target\n",
    "        val_past_cov (TimeSeries): validation past covariates\n",
    "        seed (int): random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        float: validation loss\n",
    "    \"\"\"\n",
    "    n_stacks, n_blocks, layer_width, epochs, val_wait = map(int, params)\n",
    "    quantiles = [0.1, 0.5, 0.9]\n",
    "    model = NBEATSModel(input_chunk_length=input_length, \n",
    "                        output_chunk_length=output_length,\n",
    "                        num_stacks=n_stacks,\n",
    "                        num_blocks=n_blocks,\n",
    "                        layer_widths=layer_width,\n",
    "                        n_epochs=epochs,\n",
    "                        likelihood=QuantileRegression(quantiles),\n",
    "                        optimizer_kwargs={\"lr\": 1e-3},\n",
    "                        generic_architecture=True,\n",
    "                        trend_polynomial_degree=2,\n",
    "                        random_state=seed,\n",
    "                        nr_epochs_val_period=val_wait,\n",
    "                        )\n",
    "    model.fit(series=train_target,\n",
    "              past_covariates=train_past_cov,\n",
    "              epochs=epochs,\n",
    "              verbose=True)\n",
    "    preds_val = model.predict(n=2,\n",
    "                              series=test_target_input,\n",
    "                              past_covariates=test_past_cov)\n",
    "    \n",
    "    return np.mean([rmse(target[-2:], pred) for target, pred in zip(test_target, preds_val)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "space = [\n",
    "         Integer(2, 5, name='num_stacks'),\n",
    "         Integer(4, 10, name='num_blocks'),\n",
    "         Integer(64, 256, name='layer_width'),\n",
    "         Integer(50, 100, name='n_epochs'),\n",
    "         Integer(1, 3, name='nr_epochs_val_period')]\n",
    "\n",
    "# Define the objective function to optimize\n",
    "@use_named_args(space)\n",
    "def objective(num_stacks, num_blocks, layer_width, n_epochs, nr_epochs_val_period):\n",
    "    params = (num_stacks, num_blocks, layer_width, n_epochs, nr_epochs_val_period)\n",
    "    return objective_nbeats(params, train_target, train_past_cov)\n",
    "\n",
    "# Use gp_minimize to optimize the objective function\n",
    "result = gp_minimize(objective, space, n_calls=50)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best parameters: \", result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = len(train_past_cov[0])\n",
    "output_length = len(train_future_cov[0])\n",
    "target_length = len(train_target[0])\n",
    "input_length, output_length, target_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import XGBModel\n",
    "\n",
    "xgb_model = XGBModel(\n",
    "    lags=input_length,\n",
    "    lags_past_covariates=input_length,\n",
    "    lags_future_covariates=[output_length],\n",
    "    output_chunk_length=output_length,\n",
    "    likelihood='quantile',\n",
    "    n_jobs=-1,\n",
    "    quantiles=[0.05, 0.5, 0.95],\n",
    "    random_state=seed,\n",
    "    multi_models=False,\n",
    ")\n",
    "\n",
    "xgb_predictor = xgb_model.fit(series=train_target,\n",
    "                              past_covariates=train_past_cov,\n",
    "                              future_covariates=train_future_cov,\n",
    "                              verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cf6aa321cd170048ea726af1e077df50d8012081dcfa0e535ddf51cbb8111d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
